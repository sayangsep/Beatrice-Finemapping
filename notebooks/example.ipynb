{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3353f86",
   "metadata": {},
   "source": [
    "# BEATRICE Fine-Mapping Analysis: A Comprehensive Workflow\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete fine-mapping analysis workflow using BEATRICE (Bayesian fine-mapping with trans-ethnic model selection), a state-of-the-art statistical method for identifying causal variants from genome-wide association study (GWAS) summary statistics.\n",
    "\n",
    "## Data Requirements\n",
    "\n",
    "This analysis requires:\n",
    "1. **GWAS Summary Statistics**: Effect sizes, standard errors, and p-values\n",
    "2. **Linkage Disequilibrium (LD) Matrix**: Correlation structure between variants\n",
    "\n",
    "## Analysis Workflow\n",
    "\n",
    "1. **Data Loading & Preprocessing**: Load and format GWAS summary statistics\n",
    "2. **LD Matrix Acquisition**: Download and process LD matrices from reference panels\n",
    "3. **Allele Harmonization**: Ensure consistent allele coding between datasets\n",
    "4. **Quality Control**: Filter variants and validate data integrity\n",
    "5. **Fine-Mapping Analysis**: Run BEATRICE algorithm\n",
    "6. **Results Interpretation**: Analyze posterior probabilities and credible sets\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea90786",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "### Required Libraries\n",
    "This section imports all necessary Python libraries for the analysis. Key dependencies include:\n",
    "- **pandas**: Data manipulation and analysis\n",
    "- **scipy**: Scientific computing, particularly for sparse matrix operations\n",
    "- **numpy**: Numerical computing (imported later)\n",
    "- **s3fs**: AWS S3 filesystem interface\n",
    "- **gzip**: Compressed file handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1661d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required libraries imported successfully\n",
      "✓ Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Scientific computing libraries\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Cloud storage and data access\n",
    "import s3fs\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All required libraries imported successfully\")\n",
    "print(\"✓ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b18f77",
   "metadata": {},
   "source": [
    "## 2. Allele Harmonization Functions\n",
    "\n",
    "### Comprehensive Allele Matching\n",
    "\n",
    "One of the most critical steps in fine-mapping is ensuring that alleles are properly matched between the GWAS summary statistics and the LD reference panel. This function handles multiple scenarios:\n",
    "\n",
    "**Challenge**: Summary statistics and LD reference panels may have:\n",
    "- Different allele coding (A/T vs T/A)\n",
    "- Different strand orientations (requiring complement matching)\n",
    "- Flipped effect allele designations (requiring sign flipping)\n",
    "\n",
    "**Solution**: The `comprehensive_allele_matching` function systematically checks:\n",
    "1. **Direct Match**: A1=allele1, A2=allele2 (no adjustment needed)\n",
    "2. **Flipped Match**: A1=allele2, A2=allele1 (flip beta sign)\n",
    "3. **Complement Match**: A1=complement(allele1), A2=complement(allele2) (strand flip, no sign flip)\n",
    "4. **Complement+Flipped**: A1=complement(allele2), A2=complement(allele1) (both strand and sign flip)\n",
    "\n",
    "This ensures maximum variant recovery while maintaining statistical accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29e0a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_allele_matching(summstats_df, ld_variants_df, complement_alleles):\n",
    "    \"\"\"\n",
    "    Comprehensive allele matching function that handles:\n",
    "    1. Direct match: A1=allele1, A2=allele2 (no sign flip)\n",
    "    2. Flipped match: A1=allele2, A2=allele1 (flip beta sign)\n",
    "    3. Complement match: A1=complement(allele1), A2=complement(allele2) (no sign flip)\n",
    "    4. Complement+flipped match: A1=complement(allele2), A2=complement(allele1) (flip beta sign)\n",
    "    \n",
    "    Args:\n",
    "        summstats_df: DataFrame with summary statistics containing CHR, POS, A1, A2, Beta columns\n",
    "        ld_variants_df: DataFrame with LD reference containing chromosome, position, allele1, allele2 columns  \n",
    "        complement_alleles: Dictionary mapping alleles to their complements\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Merged data with 'beta_flipped' column indicating corrected effect sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    summstats_work = summstats_df.copy()\n",
    "    ld_work = ld_variants_df.copy()\n",
    "    \n",
    "    # Add complement alleles to both datasets\n",
    "    summstats_work['A1_compl'] = summstats_work['A1'].map(complement_alleles)\n",
    "    summstats_work['A2_compl'] = summstats_work['A2'].map(complement_alleles)\n",
    "    \n",
    "    ld_work['allele1_compl'] = ld_work['allele1'].map(complement_alleles)\n",
    "    ld_work['allele2_compl'] = ld_work['allele2'].map(complement_alleles)\n",
    "    \n",
    "    # Initialize result list\n",
    "    matched_variants = []\n",
    "    \n",
    "    for idx, summ_row in summstats_work.iterrows():\n",
    "        chr_match = ld_work['chromosome'] == summ_row['CHR']\n",
    "        pos_match = ld_work['position'] == summ_row['POS']\n",
    "        chr_pos_match = chr_match & pos_match\n",
    "        \n",
    "        if not chr_pos_match.any():\n",
    "            continue\n",
    "            \n",
    "        ld_candidates = ld_work[chr_pos_match]\n",
    "        \n",
    "        for _, ld_row in ld_candidates.iterrows():\n",
    "            match_type = None\n",
    "            beta_multiplier = 1  # Default: no sign flip\n",
    "            \n",
    "            # Scenario 1: Direct match (A1=allele1, A2=allele2)\n",
    "            if (summ_row['A1'] == ld_row['allele1'] and summ_row['A2'] == ld_row['allele2']):\n",
    "                match_type = 'direct'\n",
    "                beta_multiplier = 1\n",
    "                \n",
    "            # Scenario 2: Flipped match (A1=allele2, A2=allele1)\n",
    "            elif (summ_row['A1'] == ld_row['allele2'] and summ_row['A2'] == ld_row['allele1']):\n",
    "                match_type = 'flipped'\n",
    "                beta_multiplier = -1\n",
    "                \n",
    "            # Scenario 3: Complement match (A1=complement(allele1), A2=complement(allele2))\n",
    "            elif (summ_row['A1'] == ld_row['allele1_compl'] and summ_row['A2'] == ld_row['allele2_compl']):\n",
    "                match_type = 'complement'\n",
    "                beta_multiplier = 1\n",
    "                \n",
    "            # Scenario 4: Complement+flipped match (A1=complement(allele2), A2=complement(allele1))\n",
    "            elif (summ_row['A1'] == ld_row['allele2_compl'] and summ_row['A2'] == ld_row['allele1_compl']):\n",
    "                match_type = 'complement_flipped'\n",
    "                beta_multiplier = -1\n",
    "                \n",
    "            if match_type is not None:\n",
    "                # Create merged row\n",
    "                merged_row = summ_row.copy()\n",
    "                \n",
    "                # Add LD variant information\n",
    "                for col in ld_row.index:\n",
    "                    if col not in merged_row.index:\n",
    "                        merged_row[col] = ld_row[col]\n",
    "                        \n",
    "                # Add match information\n",
    "                merged_row['match_type'] = match_type\n",
    "                merged_row['beta_original'] = summ_row['Beta']\n",
    "                merged_row['beta_corrected'] = summ_row['Beta'] * beta_multiplier\n",
    "                merged_row['sign_flipped'] = (beta_multiplier == -1)\n",
    "                \n",
    "                matched_variants.append(merged_row)\n",
    "                break  # Take first match for each summary stat variant\n",
    "    \n",
    "    if len(matched_variants) == 0:\n",
    "        print(\"Warning: No matching variants found!\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    result_df = pd.DataFrame(matched_variants).reset_index(drop=True)\n",
    "    \n",
    "    # Print matching statistics\n",
    "    match_counts = result_df['match_type'].value_counts()\n",
    "    print(\"Allele matching summary:\")\n",
    "    for match_type, count in match_counts.items():\n",
    "        print(f\"  {match_type}: {count} variants\")\n",
    "    print(f\"Total matched variants: {len(result_df)}\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759145c",
   "metadata": {},
   "source": [
    "## 3. GWAS Summary Statistics Loading\n",
    "\n",
    "### Data Source: UK Biobank High Cholesterol Study\n",
    "\n",
    "We utilize summary statistics from the UK Biobank high cholesterol study, which provides:\n",
    "- **Phenotype**: Self-reported high cholesterol (binary trait)\n",
    "- **Sample Size**: ~459,000 individuals of European ancestry\n",
    "- **Genome Coverage**: Genome-wide association results\n",
    "- **Reference**: [Neale et al. 2020, Nature Genetics](https://www.nature.com/articles/s41588-020-00735-5)\n",
    "\n",
    "The summary statistics include essential columns:\n",
    "- `CHR`: Chromosome\n",
    "- `POS`: Base pair position (GRCh37/hg19)\n",
    "- `SNP`: Variant identifier\n",
    "- `A1`: Effect allele\n",
    "- `A2`: Reference allele  \n",
    "- `Beta`: Effect size\n",
    "- `se`: Standard error\n",
    "- `P`: P-value\n",
    "- `N`: Sample size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22439589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Data Source: broad-alkesgroup-public-requester-pays/UKBB/disease_HI_CHOL_SELF_REP.sumstats.gz\n",
      "📄 Reference: https://www.nature.com/articles/s41588-020-00735-5\n",
      "⚠️  Note: This dataset requires AWS requester-pays access\n"
     ]
    }
   ],
   "source": [
    "# Data source URL and reference information\n",
    "data_source = \"broad-alkesgroup-public-requester-pays/UKBB/disease_HI_CHOL_SELF_REP.sumstats.gz\"\n",
    "reference_paper = \"https://www.nature.com/articles/s41588-020-00735-5\"\n",
    "\n",
    "print(f\"📊 Data Source: {data_source}\")\n",
    "print(f\"📄 Reference: {reference_paper}\")\n",
    "print(\"⚠️  Note: This dataset requires AWS requester-pays access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce245ab4",
   "metadata": {},
   "source": [
    "**Note**: Update path of summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef2834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading GWAS summary statistics...\n",
      "✓ Successfully loaded 12,007,881 variants\n",
      "✓ Chromosomes covered: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22)]\n",
      "✓ Sample size: 459,324 individuals\n",
      "\n",
      "📋 First 5 variants:\n"
     ]
    }
   ],
   "source": [
    "# Load GWAS summary statistics\n",
    "print(\"📂 Loading GWAS summary statistics...\")\n",
    "summstats_path = '/PATH/TO/UKBB_disease_HI_CHOL_SELF_REP.sumstats'\n",
    "\n",
    "try:\n",
    "    # Load the summary statistics file\n",
    "    summstats = pd.read_csv(summstats_path, sep='\\t')\n",
    "    \n",
    "    # Sort by chromosome and position for consistent ordering\n",
    "    summstats = summstats.sort_values(by=['CHR', 'POS']).reset_index(drop=True)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"✓ Successfully loaded {len(summstats):,} variants\")\n",
    "    print(f\"✓ Chromosomes covered: {sorted(summstats['CHR'].unique())}\")\n",
    "    print(f\"✓ Sample size: {summstats['N'].iloc[0]:,} individuals\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\n📋 First 5 variants:\")\n",
    "    summstats.head()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Could not find file at {summstats_path}\")\n",
    "    print(\"Please ensure the summary statistics file is downloaded and path is correct\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfcc6987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNP</th>\n",
       "      <th>CHR</th>\n",
       "      <th>POS</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>REF</th>\n",
       "      <th>EAF</th>\n",
       "      <th>Beta</th>\n",
       "      <th>se</th>\n",
       "      <th>P</th>\n",
       "      <th>N</th>\n",
       "      <th>INFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rs10399793</td>\n",
       "      <td>1</td>\n",
       "      <td>49298</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0.376220</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.85</td>\n",
       "      <td>459324</td>\n",
       "      <td>0.342797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rs2462492</td>\n",
       "      <td>1</td>\n",
       "      <td>54676</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>0.599409</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.11</td>\n",
       "      <td>459324</td>\n",
       "      <td>0.340158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rs3107975</td>\n",
       "      <td>1</td>\n",
       "      <td>55326</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0.991552</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.73</td>\n",
       "      <td>459324</td>\n",
       "      <td>0.324228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rs74447903</td>\n",
       "      <td>1</td>\n",
       "      <td>57033</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.84</td>\n",
       "      <td>459324</td>\n",
       "      <td>0.296256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1:70728_C_T</td>\n",
       "      <td>1</td>\n",
       "      <td>70728</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>0.997834</td>\n",
       "      <td>-0.009219</td>\n",
       "      <td>0.011699</td>\n",
       "      <td>0.38</td>\n",
       "      <td>459324</td>\n",
       "      <td>0.365713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SNP  CHR    POS A1 A2 REF       EAF      Beta        se     P  \\\n",
       "0   rs10399793    1  49298  T  C   T  0.376220 -0.000217  0.001172  0.85   \n",
       "1    rs2462492    1  54676  C  T   C  0.599409  0.001789  0.001161  0.11   \n",
       "2    rs3107975    1  55326  T  C   T  0.991552 -0.001885  0.006462  0.73   \n",
       "3   rs74447903    1  57033  T  C   T  0.998221 -0.002131  0.014378  0.84   \n",
       "4  1:70728_C_T    1  70728  C  T   C  0.997834 -0.009219  0.011699  0.38   \n",
       "\n",
       "        N      INFO  \n",
       "0  459324  0.342797  \n",
       "1  459324  0.340158  \n",
       "2  459324  0.324228  \n",
       "3  459324  0.296256  \n",
       "4  459324  0.365713  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summstats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaff54b",
   "metadata": {},
   "source": [
    "## 4. Region Selection and Quality Control\n",
    "\n",
    "### Target Region Definition\n",
    "\n",
    "For this demonstration, we focus on a specific genomic region with significant associations:\n",
    "\n",
    "**Region Parameters:**\n",
    "- **Chromosome**: 1\n",
    "- **Position Range**: 1 - 3,000,000 bp (first 3Mb of chr1)\n",
    "- **Significance Threshold**: P-value ≤ 0.01\n",
    "\n",
    "**Rationale**: \n",
    "- This region size balances computational efficiency with biological relevance\n",
    "- The p-value threshold captures potentially interesting associations while maintaining manageable dataset size\n",
    "- Fine-mapping is most effective in regions with clear association signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c41d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Applying region filters:\n",
      "   - Chromosome: 1\n",
      "   - Position range: 1 - 3,000,000 bp\n",
      "   - P-value threshold: ≤ 0.01\n",
      "   - Total variants before filtering: 12,007,881\n"
     ]
    }
   ],
   "source": [
    "# Define filtering criteria for target region\n",
    "CHROMOSOME = 1\n",
    "MAX_POSITION = 3_000_000  # 1Mb\n",
    "P_VALUE_THRESHOLD = 0.01\n",
    "\n",
    "print(f\"🎯 Applying region filters:\")\n",
    "print(f\"   - Chromosome: {CHROMOSOME}\")\n",
    "print(f\"   - Position range: 1 - {MAX_POSITION:,} bp\")\n",
    "print(f\"   - P-value threshold: ≤ {P_VALUE_THRESHOLD}\")\n",
    "print(f\"   - Total variants before filtering: {len(summstats):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c5c3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Filtering complete:\n",
      "   - Variants in target region: 523\n",
      "   - Most significant P-value: 7.70e-05\n"
     ]
    }
   ],
   "source": [
    "# Apply filters to extract target region\n",
    "subset_summstats = summstats[\n",
    "    (summstats['CHR'] == CHROMOSOME) & \n",
    "    (summstats['POS'] <= MAX_POSITION) & \n",
    "    (summstats['P'] <= P_VALUE_THRESHOLD)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Report filtering results\n",
    "print(f\"✓ Filtering complete:\")\n",
    "print(f\"   - Variants in target region: {len(subset_summstats):,}\")\n",
    "print(f\"   - Most significant P-value: {subset_summstats['P'].min():.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5cbcf",
   "metadata": {},
   "source": [
    "## 5. Linkage Disequilibrium (LD) Matrix Acquisition\n",
    "\n",
    "### UK Biobank LD Reference Panel\n",
    "\n",
    "Linkage disequilibrium matrices are essential for fine-mapping as they capture the correlation structure between variants. We use pre-computed LD matrices from the UK Biobank:\n",
    "\n",
    "**Data Source**: [AWS Marketplace - UK Biobank LD Matrices](https://aws.amazon.com/marketplace/pp/prodview-4bhcvjnh4b4cs#overview)\n",
    "\n",
    "**Key Features:**\n",
    "- **Population**: European ancestry (matching our GWAS data)\n",
    "- **Sample Size**: ~400,000 individuals\n",
    "- **Format**: Sparse matrices (.npz) with variant metadata (.gz)\n",
    "- **Coverage**: Genome-wide, segmented by chromosomal regions\n",
    "- **Coordinate System**: GRCh37/hg19 (matching summary statistics)\n",
    "\n",
    "**File Structure:**\n",
    "- `*.npz`: Compressed sparse LD matrix (lower triangular)\n",
    "- `*.gz`: Variant metadata (chromosome, position, alleles)\n",
    "\n",
    "This ensures ancestry matching between GWAS and LD reference, critical for accurate fine-mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1cba577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Configuring AWS S3 access for LD matrices...\n",
      "📡 Accessing UK Biobank LD reference panel...\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 Configuration for LD Matrix Access\n",
    "# Data source: https://aws.amazon.com/marketplace/pp/prodview-4bhcvjnh4b4cs#overview\n",
    "\n",
    "print(\"🌐 Configuring AWS S3 access for LD matrices...\")\n",
    "print(\"📡 Accessing UK Biobank LD reference panel...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad17dfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Target LD files:\n",
      "   - Matrix: broad-alkesgroup-ukbb-ld/UKBB_LD/chr1_1_3000001.npz\n",
      "   - Metadata: broad-alkesgroup-ukbb-ld/UKBB_LD/chr1_1_3000001.gz\n",
      "⬇️ Downloading LD matrix...\n",
      "✓ LD matrix loaded: [20609 20609] dimensions\n",
      "⬇️ Downloading variant metadata...\n",
      "✓ Variant metadata loaded: 20,609 variants\n",
      "✓ Position range: 10,177 - 2,999,890\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize S3 filesystem for anonymous access\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Define S3 paths for target genomic region (chr1: 1-3MB)\n",
    "ld_npz_s3_path = 'broad-alkesgroup-ukbb-ld/UKBB_LD/chr1_1_3000001.npz'\n",
    "ld_gz_s3_path = 'broad-alkesgroup-ukbb-ld/UKBB_LD/chr1_1_3000001.gz'\n",
    "\n",
    "print(f\"📁 Target LD files:\")\n",
    "print(f\"   - Matrix: {ld_npz_s3_path}\")\n",
    "print(f\"   - Metadata: {ld_gz_s3_path}\")\n",
    "\n",
    "try:\n",
    "    # Load LD matrix (sparse format)\n",
    "    print(\"⬇️ Downloading LD matrix...\")\n",
    "    ld_data_dict = {}\n",
    "    \n",
    "    with fs.open(ld_npz_s3_path, 'rb') as f:\n",
    "        ld_data_npz = np.load(f, allow_pickle=True)\n",
    "        # Extract all data while file is still open\n",
    "        for key in ld_data_npz.files:\n",
    "            ld_data_dict[key] = ld_data_npz[key]\n",
    "    \n",
    "    print(f\"✓ LD matrix loaded: {ld_data_dict['shape']} dimensions\")\n",
    "    \n",
    "    # Load variant metadata\n",
    "    print(\"⬇️ Downloading variant metadata...\")\n",
    "    with fs.open(ld_gz_s3_path, 'rb') as f:\n",
    "        with gzip.open(f, 'rt') as gz_file:\n",
    "            ld_variants_df = pd.read_csv(gz_file, sep='\\t')\n",
    "    \n",
    "    print(f\"✓ Variant metadata loaded: {len(ld_variants_df):,} variants\")\n",
    "    print(f\"✓ Position range: {ld_variants_df['position'].min():,} - {ld_variants_df['position'].max():,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading LD data: {str(e)}\")\n",
    "    print(\"Please check internet connection and S3 access permissions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8d6cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LD matrix components:\n",
      "   - row: (212375732,) ndarray\n",
      "   - col: (212375732,) ndarray\n",
      "   - format: () ndarray\n",
      "   - shape: (2,) ndarray\n",
      "   - data: (212375732,) ndarray\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['row', 'col', 'format', 'shape', 'data'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect LD matrix structure\n",
    "print(\"🔍 LD matrix components:\")\n",
    "for key in ld_data_dict.keys():\n",
    "    data = ld_data_dict[key]\n",
    "    if hasattr(data, 'shape'):\n",
    "        print(f\"   - {key}: {data.shape} {type(data).__name__}\")\n",
    "    else:\n",
    "        print(f\"   - {key}: {data} {type(data).__name__}\")\n",
    "\n",
    "# The sparse matrix format uses Coordinate (COO) format with:\n",
    "# - 'data': correlation values\n",
    "# - 'row'/'col': matrix indices \n",
    "# - 'shape': matrix dimensions\n",
    "ld_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21ce475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 LD Reference Panel Variants (first 5):\n",
      "Total variants in LD panel: 20,609\n",
      "Columns: ['rsid', 'chromosome', 'position', 'allele1', 'allele2']\n",
      "\n",
      "First 5 variants:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsid</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>position</th>\n",
       "      <th>allele1</th>\n",
       "      <th>allele2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rs367896724</td>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>A</td>\n",
       "      <td>AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rs201106462</td>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>T</td>\n",
       "      <td>TA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rs534229142</td>\n",
       "      <td>1</td>\n",
       "      <td>10511</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1:10616_CCGCCGTTGCAAAGGCGCGCCG_C</td>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>CCGCCGTTGCAAAGGCGCGCCG</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rs575272151</td>\n",
       "      <td>1</td>\n",
       "      <td>11008</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               rsid  chromosome  position  \\\n",
       "0                       rs367896724           1     10177   \n",
       "1                       rs201106462           1     10352   \n",
       "2                       rs534229142           1     10511   \n",
       "3  1:10616_CCGCCGTTGCAAAGGCGCGCCG_C           1     10616   \n",
       "4                       rs575272151           1     11008   \n",
       "\n",
       "                  allele1 allele2  \n",
       "0                       A      AC  \n",
       "1                       T      TA  \n",
       "2                       G       A  \n",
       "3  CCGCCGTTGCAAAGGCGCGCCG       C  \n",
       "4                       C       G  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine LD reference variant metadata\n",
    "print(\"📋 LD Reference Panel Variants (first 5):\")\n",
    "print(f\"Total variants in LD panel: {len(ld_variants_df):,}\")\n",
    "print(f\"Columns: {list(ld_variants_df.columns)}\")\n",
    "print(\"\\nFirst 5 variants:\")\n",
    "ld_variants_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142ea18",
   "metadata": {},
   "source": [
    "## 6. Allele Harmonization and Variant Matching\n",
    "\n",
    "### Critical Step: Ensuring Data Consistency\n",
    "\n",
    "Before fine-mapping, we must harmonize alleles between the GWAS summary statistics and LD reference panel. This process is crucial because:\n",
    "\n",
    "**Common Issues:**\n",
    "- **Strand Differences**: Same variant represented on different DNA strands (A/T vs T/A)\n",
    "- **Allele Flipping**: Effect allele designation differs between datasets\n",
    "- **Missing Variants**: Not all GWAS variants are in the LD reference\n",
    "\n",
    "**Our Approach:**\n",
    "1. **Comprehensive Matching**: Check all possible allele combinations\n",
    "2. **Sign Correction**: Flip effect size sign when alleles are swapped\n",
    "3. **Quality Control**: Report matching statistics and potential issues\n",
    "\n",
    "**Why This Matters:**\n",
    "- Incorrect allele matching leads to wrong correlation patterns\n",
    "- Sign errors reverse the direction of genetic effects\n",
    "- Missing harmonization can exclude truly causal variants\n",
    "\n",
    "The comprehensive allele matching function handles these complexities automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb638122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Starting comprehensive allele harmonization...\n",
      "   - GWAS variants: 523\n",
      "   - LD reference variants: 20,609\n",
      "   - Complement mapping: {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
      "⚙️ Processing allele matching scenarios...\n"
     ]
    }
   ],
   "source": [
    "# Define complement alleles for strand flipping\n",
    "complement_alleles = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
    "\n",
    "print(\"🧬 Starting comprehensive allele harmonization...\")\n",
    "print(f\"   - GWAS variants: {len(subset_summstats):,}\")\n",
    "print(f\"   - LD reference variants: {len(ld_variants_df):,}\")\n",
    "print(f\"   - Complement mapping: {complement_alleles}\")\n",
    "print(\"⚙️ Processing allele matching scenarios...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4134a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele matching summary:\n",
      "  direct: 523 variants\n",
      "Total matched variants: 523\n",
      "\n",
      "✅ Allele harmonization complete!\n",
      "   - Successfully matched variants: 523\n",
      "   - Matching rate: 100.0%\n",
      "\n",
      "📊 Harmonized dataset shape: (523, 27)\n",
      "📋 First few harmonized variants:\n",
      "           SNP  CHR     POS A1 A2 REF       EAF      Beta        se       P  \\\n",
      "0   rs12562034    1  768448  G  A   G  0.893631  0.003343  0.001070  0.0016   \n",
      "1  rs184266993    1  783711  G  A   G  0.994399  0.015373  0.004855  0.0020   \n",
      "2   rs58233623    1  825125  C  A   C  0.998967 -0.028689  0.010606  0.0053   \n",
      "3  rs190857891    1  854773  C  T   C  0.996782 -0.022571  0.007060  0.0016   \n",
      "4  rs374472080    1  922812  C  T   C  0.996783  0.019318  0.006105  0.0011   \n",
      "\n",
      "   ...  position  allele1 allele2 allele1_compl  allele2_compl match_type  \\\n",
      "0  ...    768448        G       A             C              T     direct   \n",
      "1  ...    783711        G       A             C              T     direct   \n",
      "2  ...    825125        C       A             G              T     direct   \n",
      "3  ...    854773        C       T             G              A     direct   \n",
      "4  ...    922812        C       T             G              A     direct   \n",
      "\n",
      "   beta_original  beta_corrected sign_flipped level_0  \n",
      "0       0.003343        0.003343        False    1061  \n",
      "1       0.015373        0.015373        False    1209  \n",
      "2      -0.028689       -0.028689        False    1539  \n",
      "3      -0.022571       -0.022571        False    1853  \n",
      "4       0.019318        0.019318        False    2606  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare LD variants DataFrame with index preservation\n",
    "ld_variants_df = ld_variants_df.reset_index(drop=False)  # Keep original index for LD matrix subsetting\n",
    "\n",
    "# Execute comprehensive allele matching\n",
    "subset_summstats = comprehensive_allele_matching(\n",
    "    subset_summstats, \n",
    "    ld_variants_df, \n",
    "    complement_alleles\n",
    ")\n",
    "\n",
    "# Report harmonization results\n",
    "print(f\"\\n✅ Allele harmonization complete!\")\n",
    "print(f\"   - Successfully matched variants: {len(subset_summstats):,}\")\n",
    "print(f\"   - Matching rate: {len(subset_summstats) / len(subset_summstats):,.1%}\")\n",
    "\n",
    "if len(subset_summstats) > 0:\n",
    "    print(f\"\\n📊 Harmonized dataset shape: {subset_summstats.shape}\")\n",
    "    print(\"📋 First few harmonized variants:\")\n",
    "    print(subset_summstats.head())\n",
    "else:\n",
    "    print(\"\\n❌ Error: No variants successfully matched!\")\n",
    "    print(\"Check allele coding and coordinate systems between datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444bcaad",
   "metadata": {},
   "source": [
    "## 7. LD Matrix Processing and Subsetting\n",
    "\n",
    "### Converting Sparse to Dense Matrix\n",
    "\n",
    "The LD matrix is stored in sparse format for memory efficiency. We need to:\n",
    "\n",
    "1. **Convert to Dense**: Transform COO sparse matrix to full dense matrix\n",
    "2. **Make Symmetric**: The stored matrix is lower triangular; we create the full symmetric matrix\n",
    "3. **Subset to Matched Variants**: Extract only rows/columns for successfully matched variants\n",
    "\n",
    "**Technical Details:**\n",
    "- **Sparse Format**: Only stores non-zero correlations (memory efficient)\n",
    "- **Symmetry**: LD matrix is symmetric since LD(i,j) = LD(j,i)\n",
    "- **Diagonal**: Perfect self-correlation (r² = 1.0)\n",
    "\n",
    "This step ensures we have the correct correlation structure for fine-mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76521d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Converting LD matrix from sparse to dense format...\n",
      "   - Sparse matrix shape: (20609, 20609)\n",
      "   - Non-zero elements: 212,375,732\n",
      "   - Sparsity: 50.00%\n",
      "⚙️ Creating symmetric matrix...\n",
      "✓ Dense LD matrix created: (20609, 20609)\n",
      "✓ Matrix is symmetric: True\n",
      "✓ Diagonal values range: [1.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Process LD matrix from sparse to dense format\n",
    "print(\"⚙️ Converting LD matrix from sparse to dense format...\")\n",
    "\n",
    "# Reconstruct sparse matrix from components\n",
    "ld_matrix = coo_matrix(\n",
    "    (ld_data_dict['data'], (ld_data_dict['row'], ld_data_dict['col'])), \n",
    "    shape=ld_data_dict['shape']\n",
    ")\n",
    "\n",
    "print(f\"   - Sparse matrix shape: {ld_matrix.shape}\")\n",
    "print(f\"   - Non-zero elements: {ld_matrix.nnz:,}\")\n",
    "print(f\"   - Sparsity: {(1 - ld_matrix.nnz / (ld_matrix.shape[0] * ld_matrix.shape[1])):.2%}\")\n",
    "\n",
    "# Convert to dense matrix\n",
    "ld_matrix_dense = ld_matrix.toarray()\n",
    "\n",
    "# Make symmetric (original matrix is lower triangular)\n",
    "print(\"⚙️ Creating symmetric matrix...\")\n",
    "ld_matrix_dense = ld_matrix_dense + ld_matrix_dense.T\n",
    "\n",
    "print(f\"✓ Dense LD matrix created: {ld_matrix_dense.shape}\")\n",
    "print(f\"✓ Matrix is symmetric: {np.allclose(ld_matrix_dense, ld_matrix_dense.T)}\")\n",
    "print(f\"✓ Diagonal values range: [{ld_matrix_dense.diagonal().min():.3f}, {ld_matrix_dense.diagonal().max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfd5679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Subsetting LD matrix to matched variants...\n",
      "   - Extracting 523 variants from LD matrix\n",
      "   - Index range: 1061 - 20599\n",
      "✓ LD matrix subsetted: (523, 523)\n",
      "✓ Matrix remains symmetric: True\n",
      "✓ Correlation range: [-1.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Subset LD matrix to matched variants only\n",
    "print(\"🎯 Subsetting LD matrix to matched variants...\")\n",
    "\n",
    "# Extract indices of matched variants\n",
    "variant_indices = subset_summstats['index'].values\n",
    "print(f\"   - Extracting {len(variant_indices)} variants from LD matrix\")\n",
    "print(f\"   - Index range: {variant_indices.min()} - {variant_indices.max()}\")\n",
    "\n",
    "# Subset both rows and columns to create final LD matrix\n",
    "subset_LD = ld_matrix_dense[np.ix_(variant_indices, variant_indices)]\n",
    "\n",
    "print(f\"✓ LD matrix subsetted: {subset_LD.shape}\")\n",
    "print(f\"✓ Matrix remains symmetric: {np.allclose(subset_LD, subset_LD.T)}\")\n",
    "print(f\"✓ Correlation range: [{subset_LD.min():.3f}, {subset_LD.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "055b22a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LD Matrix Quality Check:\n",
      "   - Matrix dimensions: (523, 523)\n",
      "   - Diagonal (should be ~1.0): [1. 1. 1. 1. 1.]\n",
      "   - Off-diagonal range: [-1.000, 1.000]\n",
      "\n",
      "📊 First 4x4 submatrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00, -2.9351717e-02, -8.3325338e-03, -1.3735899e-02],\n",
       "       [-2.9351717e-02,  1.0000000e+00, -1.9033789e-03, -5.8746589e-03],\n",
       "       [-8.3325338e-03, -1.9033789e-03,  1.0000000e+00, -4.2830547e-04],\n",
       "       [-1.3735899e-02, -5.8746589e-03, -4.2830547e-04,  1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quality check: Examine LD matrix structure\n",
    "print(\"🔍 LD Matrix Quality Check:\")\n",
    "print(f\"   - Matrix dimensions: {subset_LD.shape}\")\n",
    "print(f\"   - Diagonal (should be ~1.0): {subset_LD.diagonal()[:5]}\")\n",
    "print(f\"   - Off-diagonal range: [{subset_LD[~np.eye(subset_LD.shape[0], dtype=bool)].min():.3f}, {subset_LD[~np.eye(subset_LD.shape[0], dtype=bool)].max():.3f}]\")\n",
    "\n",
    "print(\"\\n📊 First 4x4 submatrix:\")\n",
    "subset_LD[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "950a689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Consistency Validation:\n",
      "   - LD matrix variants: 523\n",
      "   - GWAS variants: 523\n",
      "✓ Dimensions match perfectly\n",
      "✓ All values are finite\n",
      "🎉 All validations passed - ready for fine-mapping!\n"
     ]
    }
   ],
   "source": [
    "# Critical validation: Ensure data consistency\n",
    "print(\"✅ Data Consistency Validation:\")\n",
    "\n",
    "# Check dimension matching\n",
    "ld_variants = subset_LD.shape[0]\n",
    "gwas_variants = subset_summstats.shape[0]\n",
    "\n",
    "print(f\"   - LD matrix variants: {ld_variants}\")\n",
    "print(f\"   - GWAS variants: {gwas_variants}\")\n",
    "\n",
    "try:\n",
    "    assert ld_variants == gwas_variants, f\"Dimension mismatch: LD={ld_variants}, GWAS={gwas_variants}\"\n",
    "    print(\"✓ Dimensions match perfectly\")\n",
    "    \n",
    "    # Additional validations\n",
    "    assert np.all(np.isfinite(subset_LD)), \"LD matrix contains non-finite values\"\n",
    "    assert np.all(np.isfinite(subset_summstats['beta_corrected'])), \"Effect sizes contain non-finite values\"\n",
    "    print(\"✓ All values are finite\")\n",
    "    \n",
    "    print(\"🎉 All validations passed - ready for fine-mapping!\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"❌ Validation failed: {e}\")\n",
    "    print(\"Please check data processing steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1eff55",
   "metadata": {},
   "source": [
    "## 8. BEATRICE Fine-Mapping Analysis\n",
    "\n",
    "### Statistical Foundation\n",
    "\n",
    "BEATRICE implements a Bayesian fine-mapping approach that:\n",
    "\n",
    "**Core Algorithm:**\n",
    "- **Bayesian Framework**: Uses prior probabilities and likelihood functions\n",
    "- **Variational Inference**: Efficient approximation to full Bayesian inference  \n",
    "- **Multiple Causal Variants**: Can identify multiple causal variants in a region\n",
    "- **Trans-ethnic Modeling**: Handles multi-ancestry populations (when applicable)\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Z-scores**: Standardized effect sizes (Beta / Standard Error)\n",
    "- **LD Matrix**: Correlation structure between variants\n",
    "- **Sample Size**: For proper statistical scaling\n",
    "\n",
    "**Key Outputs:**\n",
    "- **Posterior Inclusion Probabilities (PIPs)**: Probability each variant is causal\n",
    "- **Credible Sets**: Sets of variants likely to contain causal variants\n",
    "- **Model Selection**: Identification of the most probable causal configuration\n",
    "\n",
    "### Data Preparation for BEATRICE\n",
    "\n",
    "We convert our harmonized data into the format required by BEATRICE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab0dcc",
   "metadata": {},
   "source": [
    "# Extract sample size for statistical scaling\n",
    "print(\"📊 Extracting study parameters...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af039e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample size: 459,324 individuals\n",
      "✓ Sample size range: 459,324 - 459,324\n",
      "✓ Sample size is consistent across all variants\n"
     ]
    }
   ],
   "source": [
    "# Extract sample size (should be consistent across variants)\n",
    "num_samples = subset_summstats['N'].iloc[0]\n",
    "sample_size_range = [subset_summstats['N'].min(), subset_summstats['N'].max()]\n",
    "\n",
    "print(f\"✓ Sample size: {num_samples:,} individuals\")\n",
    "print(f\"✓ Sample size range: {sample_size_range[0]:,} - {sample_size_range[1]:,}\")\n",
    "\n",
    "# Verify consistent sample sizes\n",
    "if sample_size_range[0] == sample_size_range[1]:\n",
    "    print(\"✓ Sample size is consistent across all variants\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Sample sizes vary across variants\")\n",
    "    print(\"   This may affect fine-mapping accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e352abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Computing Z-scores for BEATRICE analysis...\n",
      "✓ Z-scores calculated using harmonized beta values\n",
      "✓ Total variants: 523\n",
      "✓ Variants with flipped signs: 0 (0.0%)\n",
      "✓ Z-score range: [-4.03, 3.88]\n",
      "✓ All Z-scores are valid\n"
     ]
    }
   ],
   "source": [
    "# Calculate Z-scores using harmonized effect sizes\n",
    "print(\"⚙️ Computing Z-scores for BEATRICE analysis...\")\n",
    "\n",
    "# Z-score = Effect Size / Standard Error\n",
    "subset_summstats['Z'] = subset_summstats['beta_corrected'] / subset_summstats['se']\n",
    "\n",
    "# Assign final LD matrix\n",
    "LD = subset_LD\n",
    "\n",
    "# Report allele harmonization impact\n",
    "flipped_variants = subset_summstats['sign_flipped'].sum()\n",
    "total_variants = len(subset_summstats)\n",
    "flip_percentage = (flipped_variants / total_variants) * 100\n",
    "\n",
    "print(f\"✓ Z-scores calculated using harmonized beta values\")\n",
    "print(f\"✓ Total variants: {total_variants:,}\")\n",
    "print(f\"✓ Variants with flipped signs: {flipped_variants:,} ({flip_percentage:.1f}%)\")\n",
    "print(f\"✓ Z-score range: [{subset_summstats['Z'].min():.2f}, {subset_summstats['Z'].max():.2f}]\")\n",
    "\n",
    "# Quality check\n",
    "invalid_z = np.sum(~np.isfinite(subset_summstats['Z']))\n",
    "if invalid_z > 0:\n",
    "    print(f\"⚠️ Warning: {invalid_z} variants have invalid Z-scores\")\n",
    "else:\n",
    "    print(\"✓ All Z-scores are valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2666a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Preparing BEATRICE input files...\n",
      "✓ Created temporary directory: temp/\n",
      "✓ Prepared 523 variants for analysis\n",
      "✓ Data columns: ['SNP', 'Z']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare data for BEATRICE export\n",
    "print(\"📁 Preparing BEATRICE input files...\")\n",
    "\n",
    "# Create temporary directory for input files\n",
    "temp_dir = 'temp'\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "print(f\"✓ Created temporary directory: {temp_dir}/\")\n",
    "\n",
    "# Prepare Z-score data (SNP ID and Z-score only)\n",
    "beatrice_data = subset_summstats[['SNP', 'Z']].copy()\n",
    "\n",
    "print(f\"✓ Prepared {len(beatrice_data)} variants for analysis\")\n",
    "print(f\"✓ Data columns: {list(beatrice_data.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee49490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Exporting input files...\n",
      "✓ Z-scores exported: temp/data.z\n",
      "✓ LD matrix exported: temp/data.ld\n",
      "📊 File sizes:\n",
      "   - Z-scores: 0.01 MB\n",
      "   - LD matrix: 6.66 MB\n",
      "✅ Ready for BEATRICE analysis!\n"
     ]
    }
   ],
   "source": [
    "# Export files for BEATRICE analysis\n",
    "print(\"💾 Exporting input files...\")\n",
    "\n",
    "# Export Z-score file (space-separated, no header)\n",
    "z_file = f\"{temp_dir}/data.z\"\n",
    "beatrice_data.to_csv(z_file, sep=' ', index=False, header=False)\n",
    "print(f\"✓ Z-scores exported: {z_file}\")\n",
    "\n",
    "# Export LD matrix file (space-separated, high precision)\n",
    "ld_file = f\"{temp_dir}/data.ld\"\n",
    "np.savetxt(ld_file, LD, fmt='%.18e', delimiter=' ')\n",
    "print(f\"✓ LD matrix exported: {ld_file}\")\n",
    "\n",
    "# Verify file creation\n",
    "z_size = os.path.getsize(z_file) / (1024**2)  # MB\n",
    "ld_size = os.path.getsize(ld_file) / (1024**2)  # MB\n",
    "\n",
    "print(f\"📊 File sizes:\")\n",
    "print(f\"   - Z-scores: {z_size:.2f} MB\")\n",
    "print(f\"   - LD matrix: {ld_size:.2f} MB\")\n",
    "print(\"✅ Ready for BEATRICE analysis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ea069",
   "metadata": {},
   "source": [
    "### Running BEATRICE Analysis\n",
    "\n",
    "Now we execute the BEATRICE fine-mapping algorithm with our prepared data:\n",
    "\n",
    "**Command Parameters:**\n",
    "- `--z`: Z-score file (variants and effect sizes)\n",
    "- `--LD`: LD matrix file (correlation structure)\n",
    "- `--target`: Output directory for results\n",
    "- `--N`: Sample size for proper statistical scaling\n",
    "- `--plot_loss`: Generate convergence plots for diagnostics\n",
    "\n",
    "**Expected Outputs:**\n",
    "- **Posterior Inclusion Probabilities (PIPs)**: Individual variant causality probabilities\n",
    "- **Credible Sets**: Groups of variants likely to contain causal variants\n",
    "- **Convergence Plots**: Training loss and diagnostic visualizations\n",
    "- **Summary Statistics**: Analysis metadata and quality metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb383a",
   "metadata": {},
   "source": [
    "# Execute BEATRICE fine-mapping analysis\n",
    "print(\"🚀 Launching BEATRICE fine-mapping analysis...\")\n",
    "print(f\"   - Input variants: {len(beatrice_data)}\")\n",
    "print(f\"   - Sample size: {num_samples:,}\")\n",
    "print(f\"   - Output directory: results/\")\n",
    "print(\"   - Convergence plots: Enabled\")\n",
    "print(\"\\nStarting analysis... (this may take several minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a05f7860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python ../beatrice.py --plot_loss True --z temp/data.z --LD temp/data.ld --target results --N 459324\n",
      "------------------------------------------------------------\n",
      "\n",
      " Adding a constant 0.004875433165580034 to regularize LD\n",
      "100%|██████████████████████████████████████| 2001/2001 [00:04<00:00, 423.02it/s]\n",
      "GENERATING CREDIBLE SETS\n",
      "\n",
      "\n",
      " Adding a constant 0.004875433165580034 to regularize LD\n",
      "------------------------------------------------------------\n",
      "✅ BEATRICE analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Run BEATRICE with optimized parameters\n",
    "command = f\"python ../beatrice.py --plot_loss True --z {z_file} --LD {ld_file} --target results --N {num_samples}\"\n",
    "print(f\"Executing: {command}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "!python ../beatrice.py --z temp/data.z --LD temp/data.ld --target results --N {num_samples}\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✅ BEATRICE analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc24ebe",
   "metadata": {},
   "source": [
    "## 9. Results Analysis and Interpretation\n",
    "\n",
    "### Understanding BEATRICE Outputs\n",
    "\n",
    "BEATRICE generates several key output files that provide insights into the fine-mapping results:\n",
    "\n",
    "**Primary Output Files:**\n",
    "- `pip.csv`: **Posterior Inclusion Probabilities** - The probability that each variant is causal\n",
    "- `credible_set.txt`: **Credible Set Configuration** - Groups of variants that together capture causal variants with high confidence\n",
    "- `credible_set.pdf`: **Credible Set Visualization** - Graphical representation of the credible sets\n",
    "- `conditional_credible_variants_probability.txt`: **Conditional Analysis** - Advanced causality assessment\n",
    "\n",
    "**Diagnostic Files:**\n",
    "- `figures/`: Convergence plots and loss function visualizations\n",
    "- `res`: Detailed statistical output\n",
    "- `time`: Runtime performance metrics\n",
    "\n",
    "### Key Metrics Interpretation\n",
    "\n",
    "**Posterior Inclusion Probability (PIP):**\n",
    "- **Range**: 0 to 1 (probability scale)\n",
    "- **High PIP (≥0.2)**: Strong evidence for causality\n",
    "- **Medium PIP (0.05-0.2)**: Moderate evidence\n",
    "- **Low PIP (<0.05)**: Weak evidence\n",
    "\n",
    "\n",
    "Let's examine the results systematically:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3283bba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading BEATRICE results...\n",
      "✓ Loaded PIP results: 523 variants\n",
      "✓ Loaded credible set configuration\n",
      "\n",
      "📈 Summary Statistics:\n",
      "   - Maximum PIP: 0.025\n",
      "   - High confidence variants (PIP ≥ 0.2): 0\n",
      "   - Medium confidence variants (PIP 0.05-0.2): 0\n",
      "   - Total variants analyzed: 523\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze BEATRICE results\n",
    "print(\"📊 Loading BEATRICE results...\")\n",
    "\n",
    "results_dir = \"results\"\n",
    "pip_file = f\"{results_dir}/pip.csv\"\n",
    "credible_set_file = f\"{results_dir}/credible_set.txt\"\n",
    "\n",
    "try:\n",
    "    # Load PIP results\n",
    "    pip_results = pd.read_csv(pip_file)\n",
    "    print(f\"✓ Loaded PIP results: {len(pip_results)} variants\")\n",
    "    \n",
    "    # Load credible set information\n",
    "    with open(credible_set_file, 'r') as f:\n",
    "        credible_info = f.read().strip()\n",
    "    print(f\"✓ Loaded credible set configuration\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    max_pip = pip_results['pip'].max()\n",
    "    high_pip_count = (pip_results['pip'] >= 0.2).sum()\n",
    "    medium_pip_count = ((pip_results['pip'] >= 0.05) & (pip_results['pip'] < 0.2)).sum()\n",
    "    \n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"   - Maximum PIP: {max_pip:.3f}\")\n",
    "    print(f\"   - High confidence variants (PIP ≥ 0.2): {high_pip_count}\")\n",
    "    print(f\"   - Medium confidence variants (PIP 0.05-0.2): {medium_pip_count}\")\n",
    "    print(f\"   - Total variants analyzed: {len(pip_results)}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error loading results: {e}\")\n",
    "    print(\"Please ensure BEATRICE analysis completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c2cc644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Top 10 Variants by Posterior Inclusion Probability:\n",
      "======================================================================\n",
      " 1. SNP: rs7412983       | pip: 0.0253 | Confidence: LOW\n",
      " 2. SNP: rs262667        | pip: 0.0222 | Confidence: LOW\n",
      " 3. SNP: rs262649        | pip: 0.0222 | Confidence: LOW\n",
      " 4. SNP: rs3001800       | pip: 0.0222 | Confidence: LOW\n",
      " 5. SNP: rs60435231      | pip: 0.0222 | Confidence: LOW\n",
      " 6. SNP: rs55949537      | pip: 0.0222 | Confidence: LOW\n",
      " 7. SNP: rs17391750      | pip: 0.0190 | Confidence: LOW\n",
      " 8. SNP: rs12027883      | pip: 0.0190 | Confidence: LOW\n",
      " 9. SNP: rs34408665      | pip: 0.0190 | Confidence: LOW\n",
      "10. SNP: rs383968        | pip: 0.0190 | Confidence: LOW\n",
      "\n",
      "📋 Credible Set Configuration:\n",
      "==================================================\n",
      "\n",
      "\n",
      "📊 PIP Distribution:\n",
      "==============================\n",
      "   - Mean PIP: 0.0065\n",
      "   - Median PIP: 0.0063\n",
      "   - Standard deviation: 0.0049\n",
      "   - 95th percentile: 0.0158\n"
     ]
    }
   ],
   "source": [
    "# Display top variants and credible set details\n",
    "if 'pip_results' in locals():\n",
    "    print(\"🏆 Top 10 Variants by Posterior Inclusion Probability:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Sort by PIP and display top variants\n",
    "    top_variants = pip_results.nlargest(10, 'pip')\n",
    "    \n",
    "    for i, (_, variant) in enumerate(top_variants.iterrows(), 1):\n",
    "        pip_val = variant['pip']\n",
    "        confidence = \"HIGH\" if pip_val >= 0.2 else \"MEDIUM\" if pip_val >= 0.05 else \"LOW\"\n",
    "        print(f\"{i:2d}. SNP: {variant['variant_names']:15s} | pip: {pip_val:.4f} | Confidence: {confidence}\")\n",
    "    \n",
    "    print(f\"\\n📋 Credible Set Configuration:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(credible_info)\n",
    "    \n",
    "    # PIP distribution analysis\n",
    "    print(f\"\\n📊 PIP Distribution:\")\n",
    "    print(\"=\" * 30)\n",
    "    pip_values = pip_results['pip']\n",
    "    print(f\"   - Mean PIP: {pip_values.mean():.4f}\")\n",
    "    print(f\"   - Median PIP: {pip_values.median():.4f}\")\n",
    "    print(f\"   - Standard deviation: {pip_values.std():.4f}\")\n",
    "    print(f\"   - 95th percentile: {pip_values.quantile(0.95):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Results not available - please run BEATRICE analysis first\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
